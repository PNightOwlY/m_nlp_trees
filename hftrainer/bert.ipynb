{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "872d6bfe-d8d7-434d-baab-7506f5bd4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../semantic_match/models/bert-base-uncased/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "# 直接加载hugging face的checkpoint https://huggingface.co/bert-base-uncased\n",
    "# chekcpoint = \"bert-base-uncased\"\n",
    "\n",
    "# 下载到本地的路径\n",
    "checkpoint = \"../semantic_match/models/bert-base-uncased/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dd0a442-ba54-449e-94b1-41d37db7772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----output1-----\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "output2\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102,  1045,  5223,  2023,  2061,\n",
      "          2172,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(f\"-----output1-----\\n{inputs}\\n\")\n",
    "\n",
    "inputs = tokenizer(raw_inputs[0], raw_inputs[1], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(f\"output2\\n{inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4451a4cf-5db6-4136-98af-0f44114a010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----output-----\n",
      "\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1272,  0.2038,  0.0486,  ..., -0.0917,  0.0731,  0.1977],\n",
      "         [-0.0518,  0.1644,  0.0235,  ..., -1.0245,  0.1469,  0.2174],\n",
      "         [ 0.2087, -0.1733,  0.2591,  ..., -0.5546, -0.1713,  0.9268],\n",
      "         ...,\n",
      "         [ 0.2023,  0.0594,  0.4973,  ..., -0.4801, -0.4034, -0.0942],\n",
      "         [ 0.4047,  0.0502, -0.0167,  ...,  0.2565,  0.1518, -0.4275],\n",
      "         [ 0.8249,  0.1415, -0.0260,  ..., -0.0087, -0.5887, -0.3182]],\n",
      "\n",
      "        [[-0.3093,  0.4588, -0.2963,  ..., -0.1800,  0.4899,  0.4099],\n",
      "         [-0.1900,  0.5156,  0.0211,  ...,  0.0749,  0.5809,  0.6905],\n",
      "         [-0.4618,  0.9007,  0.2871,  ..., -0.1724,  0.0265, -0.3442],\n",
      "         ...,\n",
      "         [ 0.4000, -0.3200, -0.2580,  ...,  0.3174, -0.0311,  0.1128],\n",
      "         [ 0.6569,  0.1229, -0.2601,  ..., -0.0276, -0.6835, -0.2015],\n",
      "         [ 0.1426,  0.1053,  0.3697,  ...,  0.1430,  0.3493,  0.2533]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7927, -0.1932,  0.4153,  ...,  0.2445, -0.5537,  0.8210],\n",
      "        [-0.9041, -0.4203, -0.8310,  ..., -0.6397, -0.7398,  0.9234]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "\n",
      "-----ouput last_hidden_state-----\n",
      "\n",
      "tensor([[[ 0.1272,  0.2038,  0.0486,  ..., -0.0917,  0.0731,  0.1977],\n",
      "         [-0.0518,  0.1644,  0.0235,  ..., -1.0245,  0.1469,  0.2174],\n",
      "         [ 0.2087, -0.1733,  0.2591,  ..., -0.5546, -0.1713,  0.9268],\n",
      "         ...,\n",
      "         [ 0.2023,  0.0594,  0.4973,  ..., -0.4801, -0.4034, -0.0942],\n",
      "         [ 0.4047,  0.0502, -0.0167,  ...,  0.2565,  0.1518, -0.4275],\n",
      "         [ 0.8249,  0.1415, -0.0260,  ..., -0.0087, -0.5887, -0.3182]],\n",
      "\n",
      "        [[-0.3093,  0.4588, -0.2963,  ..., -0.1800,  0.4899,  0.4099],\n",
      "         [-0.1900,  0.5156,  0.0211,  ...,  0.0749,  0.5809,  0.6905],\n",
      "         [-0.4618,  0.9007,  0.2871,  ..., -0.1724,  0.0265, -0.3442],\n",
      "         ...,\n",
      "         [ 0.4000, -0.3200, -0.2580,  ...,  0.3174, -0.0311,  0.1128],\n",
      "         [ 0.6569,  0.1229, -0.2601,  ..., -0.0276, -0.6835, -0.2015],\n",
      "         [ 0.1426,  0.1053,  0.3697,  ...,  0.1430,  0.3493,  0.2533]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "-----output last_hidden_state shape-----\n",
      "\n",
      "torch.Size([2, 7, 768])\n",
      "\n",
      "-----output pool_output-----\n",
      "\n",
      "tensor([[-0.7927, -0.1932,  0.4153,  ...,  0.2445, -0.5537,  0.8210],\n",
      "        [-0.9041, -0.4203, -0.8310,  ..., -0.6397, -0.7398,  0.9234]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "\n",
      "-----output pool_output shape-----\n",
      "\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "text = [\"This is a  good hobby\", \"I dont know\"]\n",
    "inputs = tokenizer(text,  padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**inputs)\n",
    "\n",
    "print(f\"\\n-----output-----\\n\")\n",
    "print(output)\n",
    "print(f\"\\n-----ouput last_hidden_state-----\\n\")\n",
    "print(output.last_hidden_state)\n",
    "print(f\"\\n-----output last_hidden_state shape-----\\n\")\n",
    "print(output.last_hidden_state.shape)\n",
    "print(f\"\\n-----output pool_output-----\\n\")\n",
    "print(output.pooler_output)\n",
    "print(f\"\\n-----output pool_output shape-----\\n\")\n",
    "print(output.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcc192ac-ecd5-4ccc-adda-163e264073bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1272,  0.2038,  0.0486, -0.1905, -0.1351, -0.3927,  0.0782,  0.3711,\n",
      "        -0.0166, -0.1849], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.7927, -0.1932,  0.4153,  0.5269, -0.2324, -0.0410,  0.8268,  0.1612,\n",
      "         0.2632, -0.9995], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.last_hidden_state[0,0,:][:10])\n",
    "print(output.pooler_output[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6613f1-fca2-46e3-8596-b5635df1cd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75391e53-cbae-4d76-b52c-405bbe9b5951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1272,  0.2038,  0.0486, -0.1905, -0.1351, -0.3927,  0.0782,  0.3711,\n",
      "        -0.0166, -0.1849], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.7927, -0.1932,  0.4153,  0.5269, -0.2324, -0.0410,  0.8268,  0.1612,\n",
      "         0.2632, -0.9995], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292a7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
