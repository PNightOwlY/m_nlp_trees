{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2900d45-0296-4924-af0d-a8f4377e8728",
   "metadata": {},
   "source": [
    "# 中文分词技术\n",
    "中文自动分词技术主要可以归纳为三种：规则分词，统计分词，混合分词。\n",
    "## 规则分词\n",
    "\n",
    "基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句中的每个词和词表中的词逐一进行匹配，找到则切分，否者不予切分。这种方式需要通过人工建立词库，并不断更新词库，否则对于新时代出现的词将无法识别。\n",
    "按照切分的方式，主要有正向最大匹配法、逆向最大匹配法、双向最大匹配法。\n",
    "## 正向最大匹配法\n",
    "正向最大匹配法（Maximum Match Method，简称MM法）的基本思想是：假定分词词典里的最长词的长度为k，那么就用待处理文档的当前长度为k的字符串和词典进行匹配，如果找到则将分词，否则去掉最后一个字符在进行匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3e66d5d3-34e6-444c-b41b-06596332761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.max_len = self.get_max_len()\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.dict.add(word)\n",
    "        if len(word) > self.max_len:\n",
    "            self.max_len = len(word)\n",
    "        \n",
    "    \n",
    "    def get_max_len(self):\n",
    "        max_len = 0\n",
    "        for word in self.dict:\n",
    "            if len(word) > max_len:\n",
    "                max_len = len(word)\n",
    "        return max_len\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        doc_len = len(doc)\n",
    "        # 下标0开始\n",
    "        start_index = 0\n",
    "        tokens = []\n",
    "        while start_index < doc_len:\n",
    "            # 当前最长的字符串候选\n",
    "            end_index = start_index +  min(doc_len - start_index, self.max_len)\n",
    "            while end_index > start_index:\n",
    "                span = doc[start_index: end_index]\n",
    "                if span in self.dict:\n",
    "                    tokens.append(span)\n",
    "                    start_index = end_index\n",
    "                    break\n",
    "                \n",
    "                end_index -= 1\n",
    "            \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "464821e9-c98b-465e-b1de-a05a126036e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究生', '命', '的', '起源']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "mm = MM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "mm.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2bb32-49ea-4948-8dae-48c296e98a00",
   "metadata": {},
   "source": [
    "## 逆向最大匹配法\n",
    "逆向最大匹配法（Reverse Maximum Match Method，简称RMM法）的基本思想是和正向最大匹配基本一致，只是顺序是从文档的末端开始向前扫描。\\\n",
    "统计结果表明，单纯使用正向最大匹配的错误率为1/169，而逆向最大匹配是1/245，这是由于汉语中偏正结构较多，从后向前匹配可以适当提高精确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e488f349-9d05-4ce9-a2e1-e548233123f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.max_len = self.get_max_len()\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.dict.add(word)\n",
    "        if len(word) > self.max_len:\n",
    "            self.max_len = len(word)\n",
    "        \n",
    "    \n",
    "    def get_max_len(self):\n",
    "        max_len = 0\n",
    "        for word in self.dict:\n",
    "            if len(word) > max_len:\n",
    "                max_len = len(word)\n",
    "        return max_len\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        doc_len = len(doc)\n",
    "        # 下标0开始\n",
    "        end_index = doc_len\n",
    "        tokens = []\n",
    "        while end_index > 0:\n",
    "            # 当前最长的字符串候选\n",
    "            start_index = end_index - min(end_index, self.max_len)\n",
    "            \n",
    "            while start_index < end_index:\n",
    "                span = doc[start_index: end_index]\n",
    "                # print(span)\n",
    "                if span in self.dict:\n",
    "                    tokens.append(span)\n",
    "                    end_index = start_index\n",
    "                    break\n",
    "                \n",
    "                start_index += 1\n",
    "            \n",
    "        return tokens[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d20a1863-9e12-462d-94d4-d6970990be0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究', '生命', '的', '起源']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "rmm = RMM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "rmm.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cd290-286c-4ea3-8bce-c5ccfec81f77",
   "metadata": {},
   "source": [
    "## 双向最大匹配法\n",
    "双向最大匹配法(Bi-direction Matching method)是将正向最大匹配和逆向最大匹配的结果进行融合比较得到的。\\\n",
    "经过SunM.S和Benjamin K.T研究表明: \n",
    "- 90%的中文句子，使用正向和逆向最大匹配完全重合且正确。\n",
    "- 大概9%的中文句子这两种切分的结果不一样，但必定有一个是正确的。\n",
    "- 只有1%的中文句子两种切分都失败。\n",
    "\n",
    "双向最大匹配的原则是：\n",
    "- (1)如果正反向分词结果词数不同，则取分词数量较少的那个(上例:“南京市/长江/大桥”的分词数量为3而“南京市/长江大桥”的分词数量为2，所以返回分词数量为2的)。\n",
    "- (2)如果分词结果词数相同:\n",
    "    - 1)分词结果相同，就说明没有歧义，可返回任意一个。\n",
    "    - 2)分词结果不同，返回其中单字较少的那个。比如:上述示例代码中，正向最大匹配返回的结果为“['研究生----'，' 命 ----','的 ----','起源 ----']”，其中单字个数为2个;而逆向最大匹配返回的结果为“['研究 ----'，'生命 ----','的 ----','起源----']”，其中单字个数为1个。所以返回的是逆向最大匹配的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9e20b5e6-a365-4c48-9fe2-ad126c626f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIMM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.mm = MM(corpus)\n",
    "        self.rmm = RMM(corpus)\n",
    "        \n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.mm.add_word(word)\n",
    "        self.rmm.add_word(word)\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        # 正向\n",
    "        mm_tokens = self.mm.tokenize(doc)\n",
    "        rmm_tokens = self.rmm.tokenize(doc)\n",
    "\n",
    "        if len(mm_tokens) < len(rmm_tokens):\n",
    "            return mm_tokens\n",
    "        elif len(mm_tokens) > len(rmm_tokens):\n",
    "            return rmm_tokens\n",
    "        else:\n",
    "            mm_count_dict = self.get_count_dict(mm_tokens)\n",
    "            rmm_count_dict = self.get_count_dict(rmm_tokens)\n",
    "\n",
    "            count = 1\n",
    "            while count in mm_count_dict:\n",
    "                mm_count = mm_count_dict[count]\n",
    "                rmm_count = rmm_count_dict[count]\n",
    "                \n",
    "                if mm_count == rmm_count:\n",
    "                    count += 1\n",
    "                elif mm_count > rmm_count:\n",
    "                    return rmm_tokens\n",
    "                else:\n",
    "                    return mm_tokens\n",
    "            \n",
    "        return -1\n",
    "\n",
    "    def get_count_dict(self, tokens):\n",
    "        from collections import defaultdict\n",
    "        count_dict = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            count_dict[len(token)] += 1\n",
    "        return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dfcff62f-437e-4ed7-9e14-255aa13d52cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究', '生命', '的', '起源']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "bimm = BIMM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "bimm.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8e815-0b3c-4388-84dc-583dd0d971d9",
   "metadata": {},
   "source": [
    "# 统计分词\n",
    "基于规则的分词，一般都较为简单高效，但是词典的维护是一个很庞大的工程。在网络发达的今天，网络新词层出不穷，很难通过词典覆盖到所有词。 \\\n",
    "随着大规模语料库的建立，统计机器学习方法的研究和发展，基于统计的中文分词算法逐渐成为主流。 \\\n",
    "基于统计的分词，一般需要两步操作:\n",
    "1. 建立统计语言模型\n",
    "2. 对句子进行单词划分，然后对划分结果进行概率计算，获得概率最大的分词方式。这里需要使用统计学习算法，如隐马尔可夫（HMM）、条件随机场（CRF）等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "109ce56d-71bd-4a0b-bf6f-96ee53cd7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self):\n",
    "        import os\n",
    "\n",
    "        self.model_file = './data/hmm_model.pkl'\n",
    "        self.state_list = ['B', 'M', 'E', 'S']\n",
    "        self.load_para = False \n",
    "\n",
    "    def try_load_model(self, trained):\n",
    "        if trained:\n",
    "            import pickle\n",
    "            with open(self.model_file, 'rb') as f:\n",
    "                self.A_dic = pickle.load(f)\n",
    "                self.B_dic = pickle.load(f)\n",
    "                self.Pi_dic = pickle.load(f)\n",
    "                self.load_para = True\n",
    "        else:\n",
    "            # 状态转移概率矩阵, p(o_t-1 | o_t)\n",
    "            self.A_dic = {}\n",
    "            # 发射概率矩阵, p(lambda_t | o_t)\n",
    "            self.B_dic = {}\n",
    "            # 初始概率矩阵\n",
    "            self.Pi_dic = {}\n",
    "            self.load_para = False\n",
    "\n",
    "    def train(self, path):\n",
    "        self.try_load_model(False)\n",
    "\n",
    "        # 统计状态出现的次数\n",
    "        Count_dic = {}\n",
    "\n",
    "        def init_parameters():\n",
    "            for state in self.state_list:\n",
    "                self.A_dic[state] = {s:0 for s in self.state_list} # 初始每个状态之间的转移都为0\n",
    "                self.Pi_dic[state] = 0\n",
    "                self.B_dic[state] = {} # 没有单词进行转换\n",
    "\n",
    "                Count_dic[state] = 0\n",
    "\n",
    "        def makeLabel(text):\n",
    "            if len(text) == 1:\n",
    "                return ['S']\n",
    "            else:                \n",
    "                return ['B'] + ['M'] * (len(text) - 2) + ['E']\n",
    "\n",
    "        init_parameters()\n",
    "\n",
    "        line_num = -1\n",
    "        words = set()\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # 1. 去空格\n",
    "                line_num += 1\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                word_list = [word for word in line if word != \" \"]\n",
    "                words |= set(word_list)\n",
    "                \n",
    "                line_list = line.split()\n",
    "\n",
    "                line_state = []\n",
    "                for word in line_list:\n",
    "                    line_state.extend(makeLabel(word))\n",
    "                # print(line_state, word_list)\n",
    "                assert len(line_state) == len(word_list)\n",
    "\n",
    "                for k,v in enumerate(line_state):\n",
    "                    Count_dic[v] += 1\n",
    "                    if k == 0:\n",
    "                        self.Pi_dic[v] += 1\n",
    "                    else:\n",
    "                        self.A_dic[line_state[k-1]][v] += 1\n",
    "                        self.B_dic[v][word_list[k]] = self.B_dic[v].get(word_list[k],0) + 1\n",
    "\n",
    "            self.Pi_dic = {k: v / line_num for k, v in self.Pi_dic.items()}\n",
    "            self.A_dic = {k: {k1: v1 / Count_dic[k] for k1,v1 in v.items()} for k,v in self.A_dic.items()}\n",
    "            self.B_dic = {k: {k1: (v1 + 1) / Count_dic[k]  for k1, v1 in v.items()} for k, v in self.B_dic.items()}\n",
    "\n",
    "            import pickle\n",
    "            with open(self.model_file, 'wb') as f:\n",
    "                pickle.dump(self.A_dic, f)\n",
    "                pickle.dump(self.B_dic, f)\n",
    "                pickle.dump(self.Pi_dic,f)\n",
    "\n",
    "                \n",
    "\n",
    "    def viterbi(self, text, states, start_p, trans_p, emit_p):\n",
    "        V = [{}]\n",
    "        path = {}\n",
    "\n",
    "        for y in states:\n",
    "            V[0][y] = start_p[y] * emit_p[y].get(text[0],0)\n",
    "            path[y] = [y]\n",
    "        \n",
    "\n",
    "        for t in range(1, len(text)):\n",
    "            V.append({})\n",
    "            newpath = {}\n",
    "            never_seen = text[t] not in emit_p['S'].keys() and \\\n",
    "                text[t] not in emit_p['E'].keys() and \\\n",
    "                text[t] not in emit_p['M'].keys() and \\\n",
    "                text[t] not in emit_p['B'].keys()\n",
    "            \n",
    "\n",
    "            for y in states:\n",
    "                emitP = emit_p[y].get(text[t], 0) if not never_seen else 1.0\n",
    "        \n",
    "                (prob, state) = max(\n",
    "                    [(V[t-1][y0] * trans_p[y0].get(y, 0) * emitP, y0) for y0 in states if V[t-1][y0] > 0]\n",
    "                )\n",
    "                V[t][y] = prob\n",
    "                newpath[y] = path[state] + [y]\n",
    "            \n",
    "            path = newpath\n",
    "        \n",
    "        if emit_p['M'].get(text[-1], 0) > emit_p['S'].get(text[-1], 0):\n",
    "            (prob, state) = max([(V[len(text)-1][y], y) for y in ('E', 'M')])\n",
    "        else:\n",
    "            (prob, state) = max([(V[len(text)-1][y], y) for y in states])\n",
    "\n",
    "        return (prob, path[state])\n",
    "\n",
    "    def cut(self, text):\n",
    "        import os\n",
    "        if not self.load_para:\n",
    "            self.try_load_model(os.path.exists(self.model_file))\n",
    "        prob, pos_list = self.viterbi(text, self.state_list, self.Pi_dic, self.A_dic, self.B_dic)\n",
    "\n",
    "        begin, next= 0, 0\n",
    "        for i, char in enumerate(text):\n",
    "            pos = pos_list[i]\n",
    "            if pos == 'B':\n",
    "                begin = i\n",
    "            elif pos == 'E':\n",
    "                yield text[begin: i+1]\n",
    "                next = i+1\n",
    "            elif pos == 'S':\n",
    "                yield char\n",
    "                next = i+1\n",
    "        if next < len(text):\n",
    "            yield text[next:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b556760b-5c74-4b55-ad5c-6787994f385d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3588079757.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[205], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(str(list(res))\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "hmm = HMM()\n",
    "hmm.train('./data/trainCorpus.txt_utf8.txt')\n",
    "text = \"这是一个非常棒的方案！\"\n",
    "res = hmm.cut(text)\n",
    "print(str(list(res)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dac78f-bb93-4292-93b3-78b4f330f3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3a6d2-24b4-4e60-bbba-1b49093beedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
