{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2900d45-0296-4924-af0d-a8f4377e8728",
   "metadata": {},
   "source": [
    "# 中文分词技术\n",
    "中文自动分词技术主要可以归纳为三种：规则分词，统计分词，混合分词。\n",
    "## 规则分词\n",
    "\n",
    "基于规则的分词是一种机械分词方法，主要是通过维护词典，在切分语句时，将语句中的每个词和词表中的词逐一进行匹配，找到则切分，否者不予切分。这种方式需要通过人工建立词库，并不断更新词库，否则对于新时代出现的词将无法识别。\n",
    "按照切分的方式，主要有正向最大匹配法、逆向最大匹配法、双向最大匹配法。\n",
    "## 正向最大匹配法\n",
    "正向最大匹配法（Maximum Match Method，简称MM法）的基本思想是：假定分词词典里的最长词的长度为k，那么就用待处理文档的当前长度为k的字符串和词典进行匹配，如果找到则将分词，否则去掉最后一个字符在进行匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e66d5d3-34e6-444c-b41b-06596332761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.max_len = self.get_max_len()\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.dict.add(word)\n",
    "        if len(word) > self.max_len:\n",
    "            self.max_len = len(word)\n",
    "        \n",
    "    \n",
    "    def get_max_len(self):\n",
    "        max_len = 0\n",
    "        for word in self.dict:\n",
    "            if len(word) > max_len:\n",
    "                max_len = len(word)\n",
    "        return max_len\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        doc_len = len(doc)\n",
    "        # 下标0开始\n",
    "        start_index = 0\n",
    "        tokens = []\n",
    "        while start_index < doc_len:\n",
    "            # 当前最长的字符串候选\n",
    "            end_index = start_index +  min(doc_len - start_index, self.max_len)\n",
    "            while end_index > start_index:\n",
    "                span = doc[start_index: end_index]\n",
    "                if span in self.dict:\n",
    "                    tokens.append(span)\n",
    "                    start_index = end_index\n",
    "                    break\n",
    "                \n",
    "                end_index -= 1\n",
    "            \n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "464821e9-c98b-465e-b1de-a05a126036e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究生', '命', '的', '起源']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "mm = MM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "mm.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2bb32-49ea-4948-8dae-48c296e98a00",
   "metadata": {},
   "source": [
    "## 逆向最大匹配法\n",
    "逆向最大匹配法（Reverse Maximum Match Method，简称RMM法）的基本思想是和正向最大匹配基本一致，只是顺序是从文档的末端开始向前扫描。\\\n",
    "统计结果表明，单纯使用正向最大匹配的错误率为1/169，而逆向最大匹配是1/245，这是由于汉语中偏正结构较多，从后向前匹配可以适当提高精确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e488f349-9d05-4ce9-a2e1-e548233123f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.max_len = self.get_max_len()\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.dict.add(word)\n",
    "        if len(word) > self.max_len:\n",
    "            self.max_len = len(word)\n",
    "        \n",
    "    \n",
    "    def get_max_len(self):\n",
    "        max_len = 0\n",
    "        for word in self.dict:\n",
    "            if len(word) > max_len:\n",
    "                max_len = len(word)\n",
    "        return max_len\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        doc_len = len(doc)\n",
    "        # 下标0开始\n",
    "        end_index = doc_len\n",
    "        tokens = []\n",
    "        while end_index > 0:\n",
    "            # 当前最长的字符串候选\n",
    "            start_index = end_index - min(end_index, self.max_len)\n",
    "            \n",
    "            while start_index < end_index:\n",
    "                span = doc[start_index: end_index]\n",
    "                # print(span)\n",
    "                if span in self.dict:\n",
    "                    tokens.append(span)\n",
    "                    end_index = start_index\n",
    "                    break\n",
    "                \n",
    "                start_index += 1\n",
    "            \n",
    "        return tokens[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d20a1863-9e12-462d-94d4-d6970990be0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究', '生命', '的', '起源']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "rmm = RMM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "rmm.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709cd290-286c-4ea3-8bce-c5ccfec81f77",
   "metadata": {},
   "source": [
    "## 双向最大匹配法\n",
    "双向最大匹配法(Bi-direction Matching method)是将正向最大匹配和逆向最大匹配的结果进行融合比较得到的。\\\n",
    "经过SunM.S和Benjamin K.T研究表明: \n",
    "- 90%的中文句子，使用正向和逆向最大匹配完全重合且正确。\n",
    "- 大概9%的中文句子这两种切分的结果不一样，但必定有一个是正确的。\n",
    "- 只有1%的中文句子两种切分都失败。\n",
    "\n",
    "双向最大匹配的原则是：\n",
    "- (1)如果正反向分词结果词数不同，则取分词数量较少的那个(上例:“南京市/长江/大桥”的分词数量为3而“南京市/长江大桥”的分词数量为2，所以返回分词数量为2的)。\n",
    "- (2)如果分词结果词数相同:\n",
    "    - 1)分词结果相同，就说明没有歧义，可返回任意一个。\n",
    "    - 2)分词结果不同，返回其中单字较少的那个。比如:上述示例代码中，正向最大匹配返回的结果为“['研究生----'，' 命 ----','的 ----','起源 ----']”，其中单字个数为2个;而逆向最大匹配返回的结果为“['研究 ----'，'生命 ----','的 ----','起源----']”，其中单字个数为1个。所以返回的是逆向最大匹配的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e20b5e6-a365-4c48-9fe2-ad126c626f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIMM:\n",
    "    def __init__(self, corpus):\n",
    "        self.dict = corpus\n",
    "        self.mm = MM(corpus)\n",
    "        self.rmm = RMM(corpus)\n",
    "        \n",
    "\n",
    "    def add_word(self, word):\n",
    "        # 词表扩充\n",
    "        self.mm.add_word(word)\n",
    "        self.rmm.add_word(word)\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        # 正向\n",
    "        mm_tokens = self.mm.tokenize(doc)\n",
    "        rmm_tokens = self.rmm.tokenize(doc)\n",
    "\n",
    "        if len(mm_tokens) < len(rmm_tokens):\n",
    "            return mm_tokens\n",
    "        elif len(mm_tokens) > len(rmm_tokens):\n",
    "            return rmm_tokens\n",
    "        else:\n",
    "            mm_count_dict = self.get_count_dict(mm_tokens)\n",
    "            rmm_count_dict = self.get_count_dict(rmm_tokens)\n",
    "\n",
    "            count = 1\n",
    "            while count in mm_count_dict:\n",
    "                mm_count = mm_count_dict[count]\n",
    "                rmm_count = rmm_count_dict[count]\n",
    "                \n",
    "                if mm_count == rmm_count:\n",
    "                    count += 1\n",
    "                elif mm_count > rmm_count:\n",
    "                    return rmm_tokens\n",
    "                else:\n",
    "                    return mm_tokens\n",
    "            \n",
    "        return -1\n",
    "\n",
    "    def get_count_dict(self, tokens):\n",
    "        from collections import defaultdict\n",
    "        count_dict = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            count_dict[len(token)] += 1\n",
    "        return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfcff62f-437e-4ed7-9e14-255aa13d52cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['研究', '生命', '的', '起源']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {\"研究\",\"研究生\",\"生命\",\"命\",\"的\",\"起源\"}\n",
    "\n",
    "bimm = BIMM(corpus)\n",
    "\n",
    "text = \"研究生命的起源\"\n",
    "bimm.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40a8eb4-73e1-4e71-b6d0-2457bba6025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "基于规则的分词，一般都较为简单高效，但是词典的维护是一个很庞大的工程。在网络发达的今天，网络新词层出不穷，很难通过词典覆盖到所有词。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
