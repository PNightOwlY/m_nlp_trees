{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baacdee-061b-4ef1-9372-9a5d1b181967",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt\n",
    "> Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It’s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b926f1-66c4-434e-9d66-e603c7db81e1",
   "metadata": {},
   "source": [
    "## 加载语料库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee78468-abc1-4200-8ab1-da92eb843c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5096a30-d7e6-4bea-a613-c5d08bd24a03",
   "metadata": {},
   "source": [
    "## 加载分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d18a742-fe6d-4498-b605-c2039c479a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168adaec-0a0d-47d5-a846-9aa8016f1146",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "计算语料库中每一个单词出现的频率\n",
    "Compute the frequencies of each word in the corpus as we do the pre-tokenization:\n",
    "\n",
    "> 在GPT的预分词算法中，每一个空格都会变转化为'Ġ'. \\\n",
    "> Reason: We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we pre-vent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding only minimal fragmentation of words across multiple vocab tokens.\n",
    ">\n",
    "原文中说的是加入空格可以提高压缩效率，像dog. dog! dog? 这种dog的多种变体可以得到优化，及在多个词汇标记种仅仅添加最少的单词片段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e89d3e-89f8-49c1-a998-33dadf1c394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    # pre-tokenize the text\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    # word is the token, and offset is the pos of the token.\n",
    "    # print(wordwords_with_offsets[0])\n",
    "    \n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8710534-d402-4e0a-879e-56ad3bd5e131",
   "metadata": {},
   "source": [
    "The next step is to compute the base vocabulary, formed by all the characters used in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d1c390-7a44-4e4d-a8d7-624c73282088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e5456-9324-4c6c-a23d-b74126379bb0",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is \"<|endoftext|>\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600aa59-054d-4d6e-8096-13bc822d6df1",
   "metadata": {},
   "source": [
    "## Special Token, <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c22def-4dfc-43fe-881e-81f6c9c1b8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " ',',\n",
       " '.',\n",
       " 'C',\n",
       " 'F',\n",
       " 'H',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y',\n",
       " 'z',\n",
       " 'Ġ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae6d3a-e376-4e6e-a759-3b26c4181d0c",
   "metadata": {},
   "source": [
    "## Spilit Each word to charactres && Compute pair frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ace3a7-3ddb-4e95-975d-0db52a5ad73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'i', 's']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "\n",
    "# see an example\n",
    "splits['This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c72f32-4b36-4c28-a718-bca940f55a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7ca4ef-a3ee-4763-9b65-4ec22336f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('Ġ', 'i'): 2\n",
      "('Ġ', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5030da-5f36-4bec-b67e-9503c747b61c",
   "metadata": {},
   "source": [
    "遍历，找到出现频率最高的pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc8f7b54-9e13-44d9-9966-84afa88c44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ġ', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ec0de-61d6-498d-a223-7b1e21259138",
   "metadata": {},
   "source": [
    "更新词表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12adb89d-3d4e-4bc1-b378-88e656ed4dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {best_pair: \"\".join(best_pair)}\n",
    "vocab.append(\"\".join(best_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92793470-6449-453d-ba82-6027ff653fa7",
   "metadata": {},
   "source": [
    "同时我们还需要合并splits词表的映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7ea396-7b69-46aa-aca1-00dd6f36d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaf0a49d-0bed-4237-b03b-8794b1b8a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(*best_pair, splits)\n",
    "print(splits[\"Ġtrained\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31517f-6f17-439d-9c5b-7fd0b89bb402",
   "metadata": {},
   "source": [
    "到现在大家应该明白整个BPE的算法流程:\n",
    "\n",
    "1. 就是将text切分为最小单元，这里的最小单元是字符；\n",
    "\n",
    "2. GPT2把空格也进行编码并合单词合并；\n",
    "\n",
    "3. 由于BPE是贪心算法，所以需要预先设定vocab_size来决定最终的词表长度，也是合并的终止条件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18dfcc80-0180-41b2-972d-e6e85f7d1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a6da20-c6a9-4ed7-9c2f-d6a8d9307b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73cbb78f-d6f3-439a-a05b-efb52a7894fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01802dd9-490e-4345-bbfc-100ec21c42a2",
   "metadata": {},
   "source": [
    "这里展示一下最终我们训练的这个tokenizer的分词效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "186f8402-76d1-4b92-a6b1-1e4e78daee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bed3f706-c5d5-4985-956e-1acc22b27882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ca2ec-1320-4b2f-8e6d-6b332a45da0d",
   "metadata": {},
   "source": [
    "> ⚠️ Our implementation will throw an error if there is an unknown character since we didn’t do anything to handle them. GPT-2 doesn’t actually have an unknown token (it’s impossible to get an unknown character when using byte-level BPE), but this could happen here because we did not include all the possible bytes in the initial vocabulary. This aspect of BPE is beyond the scope of this section, so we’ve left the details out.\n",
    "> \n",
    "> 这里还提到一个点是，如果遇到了词表中没有出现的字符是会报错的。但是GPT-2的词表是基于byte-level的，所以不可能出现这种问题，但是由于我们本demo使用的corpus并没有涵盖所有的bytes，所以还是会出现这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3f7c31b-0a4f-49b7-a8af-531dac26b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se', 'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 整体融合\n",
    "\n",
    "# 1. 加载语料库\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# 2. 加载分词器\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 3. 分词 计算词频\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    # 分词\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    # 计算词频\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "# 4. 初始化词表\n",
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "# 5. 加入special token <|endoftext|>\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "\n",
    "# 6. 词 -> 词汇表映射切分\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "\n",
    "# 7. 定义pair词频计算\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "# 8. 定义合并pair，并更新splits映射关系\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# 9. 训练\n",
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "# 10. 打印词表\n",
    "print(vocab)\n",
    "\n",
    "# 11. 分词测试\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])\n",
    "\n",
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281333e-1cb0-4971-a100-da286ebb2ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
